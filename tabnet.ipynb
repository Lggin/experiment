{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:24:01.099556Z",
     "start_time": "2025-04-06T06:23:58.739222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ],
   "id": "3013b5b9c31841eb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:02:36.081538Z",
     "start_time": "2025-04-06T06:00:10.939060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv('train07to10.csv')\n",
    "train_data2 = pd.read_csv('train11to12.csv')\n",
    "test_data = pd.read_csv('test07to10.csv')\n",
    "test_data2 = pd.read_csv('test11to12.csv')"
   ],
   "id": "5e07410de27fbaa4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:03:16.311837Z",
     "start_time": "2025-04-06T06:02:37.748846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.concat([train_data, train_data2])\n",
    "test_df = pd.concat([test_data, test_data2])"
   ],
   "id": "b73c8371b3261237",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:03:17.040865Z",
     "start_time": "2025-04-06T06:03:16.912852Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.columns",
   "id": "86cff369cd687638",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['기준년월', '_2순위카드이용금액', '이용카드수_신용체크', '마케팅동의여부', '이용카드수_신용', '수신거부여부_메일',\n",
       "       '동의여부_한도증액안내', '이용카드수_체크', '보유여부_해외겸용_본인', '보유여부_해외겸용_신용_본인',\n",
       "       ...\n",
       "       '증감율_이용건수_일시불_분기', '잔액_신판ca평균한도소진율_r3m', '변동률_RV일시불평잔', '변동률_할부평잔',\n",
       "       '증감율_이용건수_체크_분기', '변동률_일시불평잔', '증감율_이용건수_신판_분기', '증감율_이용건수_CA_분기',\n",
       "       '잔액_신판ca최대한도소진율_r3m', '혜택수혜율_R3M'],\n",
       "      dtype='object', length=431)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:06:01.130570Z",
     "start_time": "2025-04-06T06:03:17.614585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_cols = [col for col in train_df.columns if col not in [\"ID\", \"Segment\"]]\n",
    "\n",
    "X = train_df[feature_cols].copy()\n",
    "y = train_df[\"Segment\"].copy()\n",
    "\n",
    "# 타깃 라벨 인코딩\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)"
   ],
   "id": "d664d771a7b834b9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:06:27.384530Z",
     "start_time": "2025-04-06T06:06:01.740893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "X_test = test_df.copy()\n",
    "\n",
    "encoders = {}  # 각 컬럼별 encoder 저장\n",
    "\n",
    "for col in categorical_features:\n",
    "    le_train = LabelEncoder()\n",
    "    X[col] = le_train.fit_transform(X[col])\n",
    "    encoders[col] = le_train\n",
    "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
    "    if unseen_labels_val:\n",
    "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
    "    X_test[col] = le_train.transform(X_test[col])"
   ],
   "id": "ec8d9715fcb1b531",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:06:28.987823Z",
     "start_time": "2025-04-06T06:06:27.447916Z"
    }
   },
   "cell_type": "code",
   "source": "X_test = X_test[X.columns]",
   "id": "cba287e281422882",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:07:06.484221Z",
     "start_time": "2025-04-06T06:06:29.163445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = X.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ],
   "id": "6202bc4605d617b2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:07:56.831218Z",
     "start_time": "2025-04-06T06:07:06.534880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TabNet expects numpy arrays\n",
    "X_train_np = X.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = y_encoded"
   ],
   "id": "69f232a58c1137d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:13:06.783388Z",
     "start_time": "2025-04-06T06:07:57.127950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_np, y_train_np, test_size=0.2, stratify=y_train_np\n",
    ")"
   ],
   "id": "54b8e0a8a3cbb504",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:24:16.652283Z",
     "start_time": "2025-04-06T06:24:16.320083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_sub, y_train_sub)"
   ],
   "id": "2f013a0ab3e7f6b8",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.14 GiB for an array with shape (1920000, 429) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m ros \u001B[38;5;241m=\u001B[39m RandomOverSampler(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m X_resampled, y_resampled \u001B[38;5;241m=\u001B[39m \u001B[43mros\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_sub\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_sub\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\imblearn\\base.py:202\u001B[0m, in \u001B[0;36mBaseSampler.fit_resample\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfit_resample\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams):\n\u001B[0;32m    182\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Resample the dataset.\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \n\u001B[0;32m    184\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;124;03m        The corresponding label of `X_resampled`.\u001B[39;00m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit_resample(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\sklearn\\base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1387\u001B[0m     )\n\u001B[0;32m   1388\u001B[0m ):\n\u001B[1;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\imblearn\\base.py:105\u001B[0m, in \u001B[0;36mSamplerMixin.fit_resample\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m     99\u001B[0m X, y, binarize_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X_y(X, y)\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy_ \u001B[38;5;241m=\u001B[39m check_sampling_strategy(\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy, y, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampling_type\n\u001B[0;32m    103\u001B[0m )\n\u001B[1;32m--> 105\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_resample(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    107\u001B[0m y_ \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    108\u001B[0m     label_binarize(output[\u001B[38;5;241m1\u001B[39m], classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(y)) \u001B[38;5;28;01mif\u001B[39;00m binarize_y \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    109\u001B[0m )\n\u001B[0;32m    111\u001B[0m X_, y_ \u001B[38;5;241m=\u001B[39m arrays_transformer\u001B[38;5;241m.\u001B[39mtransform(output[\u001B[38;5;241m0\u001B[39m], y_)\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\imblearn\\over_sampling\\_random_over_sampler.py:202\u001B[0m, in \u001B[0;36mRandomOverSampler._fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    196\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    197\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen shrinkage is not None, X needs to contain only \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    198\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumerical data to later generate a smoothed bootstrap \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    199\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    200\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m X_resampled \u001B[38;5;241m=\u001B[39m [\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[0;32m    203\u001B[0m y_resampled \u001B[38;5;241m=\u001B[39m [y\u001B[38;5;241m.\u001B[39mcopy()]\n\u001B[0;32m    205\u001B[0m sample_indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 6.14 GiB for an array with shape (1920000, 429) and data type float64"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:13:08.874145Z",
     "start_time": "2025-04-06T06:13:08.200085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 클래스별 가중치 텐서\n",
    "# 클래스 불균형 보정\n",
    "class_counts = {\n",
    "    'A': 972,\n",
    "    'B': 144,\n",
    "    'C': 127590,\n",
    "    'D': 349242,\n",
    "    'E': 1922052\n",
    "}\n",
    "reference = class_counts['E']\n",
    "class_weights = {le_target.transform([k])[0]: reference / v for k, v in class_counts.items()}\n",
    "\n",
    "sample_weights = np.array([class_weights[y] for y in y_train_np])\n",
    "class_weights = torch.tensor([reference / class_counts[cls] for cls in le_target.classes_], dtype=torch.float)\n",
    "\n",
    "# loss 함수 설정\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)"
   ],
   "id": "728629d7e8a30c61",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:13:09.680003Z",
     "start_time": "2025-04-06T06:13:09.468338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TabNet 학습\n",
    "clf = TabNetClassifier(\n",
    "    n_d=64,\n",
    "    n_a=64,\n",
    "    n_steps=5,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")"
   ],
   "id": "a8e76556c8499e41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id000\\.conda\\envs\\condaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:13:09.727319Z",
     "start_time": "2025-04-06T06:13:09.717674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_f1_score(y_true, y_pred):\n",
    "    y_pred_classes = y_pred.argmax(axis=1)  # 확률 → 클래스\n",
    "    return f1_score(y_true, y_pred_classes, average='macro')"
   ],
   "id": "3e6b054923d3f1b4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:13:51.023466Z",
     "start_time": "2025-04-06T06:13:09.827847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf.fit(\n",
    "    X_resampled, y_resampled,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=4,\n",
    ")"
   ],
   "id": "a7805393d7c1f300",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train_sub\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_sub\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcustom_f1_score\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:242\u001B[0m, in \u001B[0;36mTabModel.fit\u001B[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001B[0m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_network()\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_network_params()\n\u001B[1;32m--> 242\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_metric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_optimizer()\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_callbacks(callbacks)\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:644\u001B[0m, in \u001B[0;36mTabModel._set_metrics\u001B[1;34m(self, metrics, eval_names)\u001B[0m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Set attributes relative to the metrics.\u001B[39;00m\n\u001B[0;32m    633\u001B[0m \n\u001B[0;32m    634\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    640\u001B[0m \n\u001B[0;32m    641\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    642\u001B[0m metrics \u001B[38;5;241m=\u001B[39m metrics \u001B[38;5;129;01mor\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_default_metric]\n\u001B[1;32m--> 644\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;66;03m# Set metric container for each sets\u001B[39;00m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metric_container_dict \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\.conda\\envs\\condaenv\\lib\\site-packages\\pytorch_tabnet\\metrics.py:519\u001B[0m, in \u001B[0;36mcheck_metrics\u001B[1;34m(metrics)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(metric, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    518\u001B[0m     val_metrics\u001B[38;5;241m.\u001B[39mappend(metric)\n\u001B[1;32m--> 519\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMetric\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    520\u001B[0m     val_metrics\u001B[38;5;241m.\u001B[39mappend(metric()\u001B[38;5;241m.\u001B[39m_name)\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ee0d0d0146d494f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0b885b60e171589"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fddaf47a8431cf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7010c290640fea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "473b22037d9e94b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
